{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Libtorch Source code.\n",
    "\n",
    "```python\n",
    "git clone https://github.com/pytorch/pytorch.git\n",
    "git submodule update --init --recursive\n",
    "\n",
    "cd pytorch\n",
    "# On Linux:\n",
    "python setup.py build\n",
    "# On macOS (may need to prefix with `MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++` when using anaconda)\n",
    "LDSHARED=\"cc -dynamiclib -undefined dynamic_lookup\" python setup.py build\n",
    "\n",
    "cd ..; mkdir -p build; cd build\n",
    "cmake .. -DPYTHON_EXECUTABLE:FILEPATH=$(which python)  # helpful if you use anaconda\n",
    "make -j\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors:\n",
    "\n",
    "In mathematics, a tensor is a geometric object that maps in a multi-linear manner geometric vectors, scalars, and other tensors to a resulting tensor. Vectors and scalars which are often used in elementary physics and engineering applications, are considered as the simplest tensors. Vectors from the dual space of the vector space, which supplies the geometric vectors, are also included as tensors.[1] Geometric in this context is chiefly meant to emphasize independence of any selection of a coordinate system.[more](https://en.wikipedia.org/wiki/Tensor)\n",
    "\n",
    "In simple terms, **Tensor** is an arrangment of numbers in different dimentions. For example\n",
    "<br>\n",
    "- Vectors/Arrays are 1D-Tensor.\n",
    "- Matrix with rows and coulmns are 2D-Tensor.\n",
    "- RGB image is an example of 3D-Tensor.  \n",
    "\n",
    "Similarly we can have 4D-Tensor, 5D-Tensor ..... N-D-Tensor etc.\n",
    "\n",
    "Throught the rest of our discussion we will see how we can use **Pytorch's python and C++ frontend** to create these **Tensors** and how we can playaround and work with Pythorch API's to implement a simple Neural net to most complicated ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor creation using Pytorch \n",
    "\n",
    "### using Python frontend:\n",
    "\n",
    "\n",
    "```python\n",
    "# First, import PyTorch\n",
    "import torch\n",
    "\n",
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "print(features)\n",
    "\n",
    "############# OUTPUT #################\n",
    "\n",
    "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using C++ frontend:\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/Context.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main()\n",
    "{\n",
    "  // Seed for Random number\n",
    "  torch::manual_seed(9);\n",
    "  torch::Tensor features = torch::rand({1, 5});\n",
    "  std::cout << \"features = \" << features << \"\\n\";\n",
    "}\n",
    "\n",
    "################ OUTPUT #######################\n",
    "\n",
    "features =  0.6558  0.3020  0.4799  0.7774  0.9180\n",
    "[ Variable[CPUFloatType]{1,5} ]\n",
    "\n",
    "```\n",
    "\n",
    "As we can see we include the headerfiles which is similar to importing a module in python.  \n",
    "**NOTE**: We are using similar API's **manual_seed() and rand()** to generate the Tensor. This is a pattern which we will see and can be used as and advantage to convert our python code to c++ or viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Shape of Tensor\n",
    "\n",
    "### Using Python:\n",
    "\n",
    "Pytoch syntax is pretty similar to Numpy.\n",
    "\n",
    "```python\n",
    "print(features.shape)\n",
    "print(features.shape[0])\n",
    "print(features.shape[1])\n",
    "\n",
    "############# OUTPUT #################\n",
    "\n",
    "torch.Size([1, 5])\n",
    "1\n",
    "5\n",
    "\n",
    "```\n",
    "\n",
    "### Using C++:\n",
    "\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/Context.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main()\n",
    "{\n",
    "  // Seed for Random number\n",
    "  torch::manual_seed(9);\n",
    "  torch::Tensor features = torch::rand({1, 5});\n",
    "  std::cout << \"features shape = \" << features.sizes() << \"\\n\";\n",
    "  std::cout << \"features shape rows = \" << features.sizes()[0] << \"\\n\";\n",
    "  std::cout << \"features shape col = \" << features.size(1) << \"\\n\";\n",
    "\n",
    "}\n",
    "\n",
    "################ OUTPUT #######################\n",
    "\n",
    "features shape = [1, 5]\n",
    "features shape rows = 1\n",
    "features shape col = 5\n",
    "\n",
    "```\n",
    "\n",
    "Notice the different ways/methods of accessing the indexes of a Tensors.\n",
    "```cpp\n",
    "  // Numpy like indexing style.\n",
    "  std::cout << \"features shape rows = \" << features.sizes()[0] << \"\\n\";\n",
    "  // C++ way of accessing the indexes.\n",
    "  std::cout << \"features shape col = \" << features.size(1) << \"\\n\";\n",
    "```\n",
    "\n",
    "In the rest of the of the discussion, we use the below version as our choice to access the shape of a **Tensor**.\n",
    "```cpp\n",
    "  std::cout << \"features shape col = \" << features.size(1) << \"\\n\";\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Net.\n",
    "\n",
    "Let us create a simple Neual Net from scratch using Pytorch. \n",
    "\n",
    "![](assets/simple_neuron.png)\n",
    "\n",
    "Mathematically this looks like: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The above mathemetical formula for $h$ can also be written in matrix form as follows.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActivationFunction(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(features.shape[0])\n",
    "print(features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5]),\n",
       " torch.Size([1, 5]),\n",
       " torch.Size([1, 1]),\n",
       " tensor([[0.1595]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ActivationFunction(torch.sum(features * weights) + bias)\n",
    "features.shape, weights.shape, bias.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5]),\n",
       " torch.Size([1, 5]),\n",
       " torch.Size([1, 1]),\n",
       " tensor([[0.1595]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ActivationFunction((features*weights).sum() + bias)\n",
    "features.shape, weights.shape, bias.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ActivationFunction(torch.mm(features,weights.view(5,1)) + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using C++:\n",
    "\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/Context.h>\n",
    "#include <iostream>\n",
    "\n",
    "torch::Tensor ActivationFunction(const torch::Tensor &x)\n",
    "{\n",
    "  // Sigmoid function.\n",
    "  auto retVal = 1/(1+torch::exp(-x));\n",
    "  return retVal;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  // Seed for Random number\n",
    "  torch::manual_seed(9);\n",
    "  //at::manual_seed(9);\n",
    "\n",
    "  torch::Tensor features = torch::rand({1, 5});\n",
    "  std::cout << \"features = \" << features << \"\\n\";\n",
    "\n",
    "  auto weights = torch::randn_like(features);\n",
    "  std::cout << \"weights = \" << weights << \"\\n\";\n",
    "\n",
    "  auto bias = torch::randn({1,1});\n",
    "  std::cout << \"bias  =\" << bias << \"\\n\";\n",
    "\n",
    "  // There are multiple ways to get the same result. Here are the few of them.\n",
    "  // 1st way\n",
    "  auto y = ActivationFunction(torch::sum(features * weights) +bias);\n",
    "  std::cout << \"y using (\\\"torch::sum()\\\") = \" << y << \"\\n\";\n",
    "\n",
    "  // 2nd way\n",
    "  y = ActivationFunction((features * weights).sum() +bias);\n",
    "  std::cout << \"y using (\\\".sum()\\\") = \" << y << \"\\n\";\n",
    "\n",
    "  // 3rd and preferred way. Using Matrix multiplication.\n",
    "  y = ActivationFunction(torch::mm(features, weights.view({5,1}))+bias);\n",
    "  std::cout << \"y using (\\\"torch::mm().view()\\\") = \" << y << \"\\n\";\n",
    "\n",
    "  // Reshaping the tensor using reshape().\n",
    "  y = ActivationFunction(torch::mm(features, weights.reshape({5,1}))+bias);\n",
    "  std::cout << \"y using (\\\"torch.mm().reshape() \\\") = \" << y << \"\\n\";\n",
    "\n",
    "  // Reshaping the tensor using inplace resize_().\n",
    "  y = ActivationFunction(torch::mm(features, weights.resize_({5,1}))+bias);\n",
    "  std::cout << \"y using (\\\"torch.mm().resize_() \\\") = \" << y << \"\\n\";\n",
    "\n",
    "  std::cout << \"features shape = \" << features.sizes() << \"\\n\";\n",
    "  std::cout << \"features shape rows = \" << features.sizes()[0] << \"\\n\";\n",
    "  std::cout << \"features shape col = \" << features.size(1) << \"\\n\";\n",
    "  \n",
    "  return 0;\n",
    "}\n",
    "\n",
    "\n",
    "################ OUTPUT #######################\n",
    "\n",
    "features =  0.6558  0.3020  0.4799  0.7774  0.9180\n",
    "[ Variable[CPUFloatType]{1,5} ]\n",
    "    \n",
    "weights = -1.3316  0.4487 -0.2635  1.2342 -1.1583\n",
    "[ Variable[CPUFloatType]{1,5} ]\n",
    "    \n",
    "bias  =-1.7026\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "y using (\"torch::sum()\") = 0.01 * 6.4732\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "y using (\".sum()\") = 0.01 * 6.4732\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "y using (\"torch::mm().view()\") = 0.01 * 6.4732\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "y using (\"torch.mm().reshape() \") = 0.01 * 6.4732\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "y using (\"torch.mm().resize_() \") = 0.01 * 6.4732\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "    \n",
    "features shape = [1, 5]\n",
    "features shape rows = 1\n",
    "features shape col = 5\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Neural Net.\n",
    "![image](./assets/multilayer_diagram_weights.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(9) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4684]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = ActivationFunction(torch.mm(features, W1) +B1)\n",
    "y = ActivationFunction(torch.mm(h, W2) + B2)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using C++\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/Context.h>\n",
    "#include <iostream>\n",
    "\n",
    "torch::Tensor ActivationFunction(const torch::Tensor &x)\n",
    "{\n",
    "  // Sigmoid function.\n",
    "  auto retVal = 1/(1+torch::exp(-x));\n",
    "  return retVal;\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "  // Seed for Random number\n",
    "  torch::manual_seed(9);\n",
    "\n",
    "  torch::Tensor features = torch::rand({1, 3});\n",
    "  //auto features = torch::rand({1, 3});\n",
    "  std::cout << \"features = \" << features << \"\\n\";\n",
    "\n",
    "  // Define the size of each layer in our network\n",
    "  // Number of input units, must match number of input features\n",
    "  auto num_inputs = features.size(1);\n",
    "  // Number of hidden units\n",
    "  int num_hidden_layers = 2;\n",
    "  // Number of output units\n",
    "  int num_output = 1;\n",
    "\n",
    "\n",
    "  // Weights for inputs to hidden layer\n",
    "  auto W1 = torch::randn({num_inputs, num_hidden_layers});\n",
    "  std::cout << \"W1 = \" << W1.sizes() << \"\\n\";\n",
    "  // Weights for hidden layer to output layer\n",
    "  auto W2 = torch::randn({num_hidden_layers, num_output});\n",
    "  std::cout << \"W2 = \" << W2.sizes() << \"\\n\";\n",
    "\n",
    "  // and bias terms for hidden and output layers\n",
    "  auto B1 = torch::randn({1, num_hidden_layers});\n",
    "  std::cout << \"B1 = \" << B1.sizes() << \"\\n\";\n",
    "  auto B2 = torch::randn({1, num_output});\n",
    "  std::cout << \"B2 = \" << B2.sizes() << \"\\n\";\n",
    "\n",
    "\n",
    "  auto h = ActivationFunction(torch::mm(features, W1) + B1);\n",
    "  auto y = ActivationFunction(torch::mm(h, W2) + B2);\n",
    "  std::cout << \"y = \" << y << \"\\n\";\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "################ OUTPUT #######################\n",
    "\n",
    "features =  0.6558  0.3020  0.4799\n",
    "[ Variable[CPUFloatType]{1,3} ]\n",
    "W1 = [3, 2]\n",
    "W2 = [2, 1]\n",
    "B1 = [1, 2]\n",
    "B2 = [1, 1]\n",
    "y =  0.6860\n",
    "[ Variable[CPUFloatType]{1,1} ]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "Download the mnist data using the [script](https://gist.github.com/goldsborough/6dd52a5e01ed73a642c1e772084bcd03)\n",
    "```\n",
    " sudo mkdir -p /opt/MNIST\n",
    " sudo python download_mnist.py -d /opt/MNIST/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "out = torch.mm(h, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "def SoftMax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = SoftMax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++\n",
    "\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/Context.h>\n",
    "#include <iostream>\n",
    "\n",
    "torch::Tensor ActivationFunction(const torch::Tensor &x)\n",
    "{\n",
    "  // Sigmoid function.\n",
    "  auto retVal = 1/(1+torch::exp(-x));\n",
    "  return retVal;\n",
    "}\n",
    "\n",
    "torch::Tensor SoftMax(const torch::Tensor &x)\n",
    "{\n",
    "  auto sum = torch::sum(torch::exp(x),1);\n",
    "  return torch::exp(x)/sum.view({-1, 1});\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "  // Download the MNIST data using the script present in,\n",
    "  // ../scripts/download_mnist.py\n",
    "  // Create a data loader for the MNIST dataset.\n",
    "  auto trainLoader = torch::data::make_data_loader(\n",
    "      torch::data::datasets::MNIST(\"/opt/MNIST/\").map(\n",
    "        torch::data::transforms::Stack<>()),\n",
    "      /*batch_size=*/64);\n",
    "\n",
    "  auto batch = std::begin(*trainLoader);\n",
    "\n",
    "  auto images = batch->data;\n",
    "  auto target = batch->target;\n",
    "  std::cout << \"images = \" << images.sizes() << \"\\n\";\n",
    "  std::cout << \"targets = \" << target.sizes() << \"\\n\";\n",
    "\n",
    "  // Seed for Random number\n",
    "  torch::manual_seed(9);\n",
    "\n",
    "  auto inputs = images.view({images.size(0),-1});\n",
    "  std::cout << \"inputs = \" << inputs.sizes() << \"\\n\";\n",
    "\n",
    "\n",
    "  // Create parameters\n",
    "  auto w1 = torch::randn({784, 256});\n",
    "  std::cout << \"w1 =\" << w1.sizes() << \"\\n\";\n",
    "  auto b1 = torch::randn({256});\n",
    "  std::cout << \"b1 =\" << b1.sizes() << \"\\n\";\n",
    "\n",
    "  auto w2 = torch::randn({256, 10});\n",
    "  std::cout << \"w2 =\" << w2.sizes() << \"\\n\";\n",
    "  auto b2 = torch::randn({10});\n",
    "  auto b2 = torch::randn({10});\n",
    "  std::cout << \"b2 =\" << b2.sizes() << \"\\n\";\n",
    "\n",
    "  auto h = ActivationFunction(torch::mm(inputs, w1) + b1);\n",
    "  std::cout << \"h =\" << h.sizes() << \"\\n\";\n",
    "  auto out = torch::mm(h, w2) + b2;\n",
    "  std::cout << \"out =\" << out.sizes() << \"\\n\";\n",
    "  //std::cout << \"out =\" << out << \"\\n\"; \n",
    "\n",
    "  auto pred = SoftMax(out);\n",
    "\n",
    "  std::cout << \"pred = \" << pred.sizes() << \"\\n\";\n",
    "  //  std::cout << \"pred = \" << pred << \"\\n\";\n",
    "\n",
    "  auto predSum = torch::sum(pred, 1);\n",
    "  std::cout << \"predsum shape = \" << predSum.sizes() << \"\\n\";\n",
    "  //std::cout << \"predsum = \" << predSum << \"\\n\";\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "##################### OUTPUT ########################\n",
    "\n",
    "images = [64, 1, 28, 28]\n",
    "targets = [64]\n",
    "inputs = [64, 784]\n",
    "w1 =[784, 256]\n",
    "b1 =[256]\n",
    "w2 =[256, 10]\n",
    "b2 =[10]\n",
    "h =[64, 256]\n",
    "out =[64, 10]\n",
    "pred = [64, 10]\n",
    "predsum shape = [64]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Mnist Example \n",
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9066940238480883\n",
      "Training loss: 0.8177144474058009\n",
      "Training loss: 0.5046289031153549\n",
      "Training loss: 0.41867202615687077\n",
      "Training loss: 0.37943710804557496\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <iostream>\n",
    "\n",
    "struct ReLu: torch::nn::Module {\n",
    "  ReLu() {}\n",
    "  torch::Tensor forward(torch::Tensor x) {\n",
    "    return torch::relu(x);\n",
    "  }\n",
    "};\n",
    "\n",
    "struct LogSoftMax : torch::nn::Module {\n",
    "  LogSoftMax() {}\n",
    "  torch::Tensor forward(torch::Tensor x) {\n",
    "    return torch::log_softmax(x, /*dim=*/1);\n",
    "  }\n",
    "};\n",
    "\n",
    "int main()\n",
    "{\n",
    "  // Train model.\n",
    "  auto trainDataLoader = torch::data::make_data_loader(\n",
    "      torch::data::datasets::MNIST(\"/opt/MNIST/\").map(\n",
    "        torch::data::transforms::Stack<>()),\n",
    "      /*batch_size=*/64);\n",
    "\n",
    "  torch::nn::Sequential sequential(torch::nn::Linear(784, 128),\n",
    "      //torch::nn::Functional(torch::relu),\n",
    "      ReLu(),\n",
    "      torch::nn::Linear(128, 64),\n",
    "      //torch::nn::Functional(torch::relu),\n",
    "      ReLu(),\n",
    "      torch::nn::Linear(64, 10),\n",
    "      LogSoftMax());\n",
    "\n",
    "  std::cout << \"Model:\\n\";\n",
    "  std::cout << c10::str(sequential) << \"\\n\\n\";\n",
    "\n",
    "  torch::optim::SGD optimizer(sequential->parameters(), /*lr=*/0.01);\n",
    "\n",
    "  std::cout << \"Training:\\n\\n\";\n",
    "  for (size_t epoch = 1; epoch <= 6; ++epoch) {\n",
    "    size_t batch_index = 0;\n",
    "    // Iterate the data loader to yield batches from the dataset.\n",
    "    for (auto& batch : *trainDataLoader) {\n",
    "\n",
    "      // Reset gradients.\n",
    "      optimizer.zero_grad();\n",
    "\n",
    "      // Execute the model on the input data.\n",
    "      auto imgs = batch.data.view({batch.data.size(0), -1});\n",
    "\n",
    "      torch::Tensor prediction = sequential->forward(imgs);\n",
    "\n",
    "      // Compute a loss value to judge the prediction of our model.\n",
    "      torch::Tensor loss = torch::nll_loss(prediction, batch.target);\n",
    "\n",
    "      // Compute gradients of the loss w.r.t. the parameters of our model.\n",
    "      loss.backward();\n",
    "\n",
    "      // Update the parameters based on the calculated gradients.\n",
    "      optimizer.step();\n",
    "\n",
    "      // Output the loss and checkpoint every 100 batches.\n",
    "      if (++batch_index % 100 == 0) {\n",
    "        std::cout << \"Epoch: \" << epoch << \" | Batch: \" << batch_index\n",
    "          << \" | Training Loss: \" << loss.item<float>() << \"\\n\\n\";\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Test the built model.\n",
    "\n",
    "  std::cout << \"Testing:\\n\\n\";\n",
    "  uint32_t batchSize = 64;\n",
    "  torch::data::datasets::MNIST::Mode  mode = torch::data::datasets::MNIST::Mode::kTest;\n",
    "  auto testDataLoader = torch::data::make_data_loader(\n",
    "      torch::data::datasets::MNIST(\"/opt/MNIST/\", mode).map(\n",
    "        torch::data::transforms::Stack<>()),\n",
    "      batchSize);\n",
    "\n",
    "  auto batch = std::begin(*testDataLoader);\n",
    "\n",
    "  auto images = batch->data;\n",
    "  auto target = batch->target;\n",
    "  //  std::cout << \"images = \" << images.sizes() << \"\\n\";\n",
    "  //  std::cout << \"targets = \" << target.sizes() << \"\\n\";\n",
    "\n",
    "  auto index = torch::randint(batchSize,{1,batchSize}, at::kInt);\n",
    "\n",
    "  std::cout << \"+---------------+---------------+-------------+\\n\";\n",
    "  std::cout << \"|  Actual value |   Prediction  |  Confidence |\\n\";\n",
    "  std::cout << \"|---------------|---------------|-------------|\\n\";\n",
    "\n",
    "  for(uint32_t i = 0; i < batchSize; i++) {\n",
    "\n",
    "    //let us predict the image results from our model.\n",
    "    auto image = images[i];\n",
    "    //std::cout << \"image = \" << image.sizes() << \"\\n\";\n",
    "\n",
    "    auto img = image.view({1,784});\n",
    "    //std::cout << \"img = \" << img.sizes() << \"\\n\";\n",
    "          // std::cout << img << \"\\n\";\n",
    "    auto logProb = sequential->forward(img);\n",
    "    //auto result = std::get<1>(sequential(img).max(/*dim=*/1));\n",
    "\n",
    "    auto prediction = torch::exp(logProb);\n",
    "    //std::cout << \"prediction = \" << prediction << \"\\n\";\n",
    "    auto maxVal = prediction.max(1);\n",
    "    std::cout << \"|\\t\" << std::get<1>(maxVal).item<int>() <<  \"\\t| \\t  \" << target[i].item<int>() << \"\\t|  \"<< std::get<0>(maxVal).item<float>() << \"   |\\n\";\n",
    "    std::cout << \"+---------------+---------------+-------------+\\n\";\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "   \n",
    "##################### OUTPUT ########################\n",
    "\n",
    "Model:\n",
    "torch::nn::Sequential(\n",
    "  (0): torch::nn::Linear(in=784, out=128, with_bias=true)\n",
    "  (1): ReLu\n",
    "  (2): torch::nn::Linear(in=128, out=64, with_bias=true)\n",
    "  (3): ReLu\n",
    "  (4): torch::nn::Linear(in=64, out=10, with_bias=true)\n",
    "  (5): LogSoftMax\n",
    ")\n",
    "\n",
    "Training:\n",
    "\n",
    "Epoch: 1 | Batch: 100 | Training Loss: 2.26577\n",
    "\n",
    "Epoch: 1 | Batch: 200 | Training Loss: 2.18934\n",
    "\n",
    "Epoch: 1 | Batch: 300 | Training Loss: 2.05134\n",
    "\n",
    "Epoch: 1 | Batch: 400 | Training Loss: 1.90906\n",
    "\n",
    "Epoch: 1 | Batch: 500 | Training Loss: 1.66898\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Testing:\n",
    "\n",
    "+---------------+---------------+-------------+\n",
    "|  Actual value |   Prediction  |  Confidence |\n",
    "|---------------|---------------|-------------|\n",
    "|\t9\t| \t  9\t|  0.898488   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t8\t| \t  5\t|  0.528291   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t0\t| \t  0\t|  0.997268   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t4\t| \t  4\t|  0.735526   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t9\t| \t  9\t|  0.938822   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t3\t| \t  3\t|  0.981706   |\n",
    "+---------------+---------------+-------------+\n",
    "|\t9\t| \t  9\t|  0.927858   |\n",
    "+---------------+---------------+-------------+\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_django",
   "language": "python",
   "name": "py37_django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
